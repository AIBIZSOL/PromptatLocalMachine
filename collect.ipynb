{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingest\n",
    "import run_localGPT\n",
    "from constants import SOURCE_DIRECTORY, PERSIST_DIRECTORY, MODEL_ID, MODEL_BASENAME, EMBEDDING_MODEL_NAME\n",
    "import os\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_type='cpu'\n",
    "chunk_size=100\n",
    "chunk_overlap=50\n",
    "dir=\"Uniswap v3/liquidity_model\" # dex_name/feature\n",
    "k=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "source_directory = os.path.join(SOURCE_DIRECTORY, dir)\n",
    "save_path = os.path.join(PERSIST_DIRECTORY, dir)\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    model_kwargs={\"device\": device_type},\n",
    "    )\n",
    "\n",
    "# change the embedding type here if you are running into issues.\n",
    "    # These are much smaller embeddings and will work for most appications\n",
    "    # If you use HuggingFaceEmbeddings, make sure to also use the same in the\n",
    "    # run_localGPT.py file.\n",
    "\n",
    "    # embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/liquidity_model\n",
      "Loaded 3 documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/liquidity_model\n",
      "Split into 753 chunks of text\n"
     ]
    }
   ],
   "source": [
    "ingest.main(device_type=device_type, embedding_model=embeddings, chunk_size=chunk_size, chunk_overlap=chunk_overlap,\\\n",
    "source_directory=source_directory, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = os.path.join(save_path, f'cs_{chunk_size}_co_{chunk_overlap}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmahmoud\\localGPT/DB\\Uniswap v3/liquidity_model\\cs_100_co_50\n"
     ]
    }
   ],
   "source": [
    "print(persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model: TheBloke/Llama-2-7b-Chat-GGUF, on: cpu\n",
      "This action can take a few minutes!\n",
      "Using Llamacpp for GGUF/GGML quantized models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = run_localGPT.load_model(device_type, model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
    "use_history = False\n",
    "query = \"\"\"Extract the liquidity model employed by the DEX. Look for any of the following:\n",
    "  - Constant Product Market Maker (CPMM)\n",
    "  - Constant Sum Market Maker (CSMM)\n",
    "  - Constant Mean Market Maker (CMMM)\n",
    "  - Hybrid Constant Function Market Makers (CFMMs)\n",
    "  - Dynamic Automated Market Maker (DAMM)\n",
    "  - Proactive Market Maker (PMM)\n",
    "  - Virtual Automated Market Makers (vAMM)\n",
    "If the information is not available in documents feel free to precise it.\n",
    "Reply with only one short sentence or word.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n",
      "Use history set to: False\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    }
   ],
   "source": [
    "answer, docs = run_localGPT.main(device_type, llm, k, persist_directory, query, use_history, verbose=True, show_sources=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting features for all DEXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama-2-13b-chat-gptq'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Llama-2-13B-chat-GPTQ\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uniswap v3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = [os.path.basename(folder) for folder in glob.glob(f\"{SOURCE_DIRECTORY}/*\")]\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uniswap v3/fees', 'Uniswap v3/liquidity_model', 'Uniswap v3/license']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = [\"fees\", \"liquidity_model\", \"license\"]\n",
    "directories = [f\"{folder}/{feature}\" for folder in folders for feature in features]\n",
    "directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [(5, 500, 200), (4, 500, 100), (5, 500, 200)] # k, chunk_size, chunk_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Uniswap v3 fees with k=5, cs=500, co=200\n",
      "Ingesting...\n",
      "Loading documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/fees\n",
      "Loaded 3 documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/fees\n",
      "Split into 111 chunks of text\n",
      "Running localGPT...\n",
      "Running on: cpu\n",
      "Use history set to: False\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Uniswap v3 liquidity_model with k=5, cs=500, co=200\n",
      "Ingesting...\n",
      "Loading documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/liquidity_model\n",
      "Loaded 3 documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/liquidity_model\n",
      "Split into 753 chunks of text\n",
      "Running localGPT...\n",
      "Running on: cpu\n",
      "Use history set to: False\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Uniswap v3 license with k=5, cs=500, co=200\n",
      "Ingesting...\n",
      "Loading documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/license\n",
      "Loaded 1 documents from C:\\Users\\mmahmoud\\localGPT/SOURCE_DOCUMENTS\\Uniswap v3/license\n",
      "Split into 63 chunks of text\n",
      "Running localGPT...\n",
      "Running on: cpu\n",
      "Use history set to: False\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "for k, cs, co in configs:\n",
    "    for dir in directories:\n",
    "        dex_name = dir.split(\"/\")[0]\n",
    "        feature = dir.split(\"/\")[1]\n",
    "\n",
    "        print(f\"Running for {dex_name} {feature} with k={k}, cs={cs}, co={co}\")\n",
    "\n",
    "        print(\"Ingesting...\")\n",
    "        source_directory = os.path.join(SOURCE_DIRECTORY, dir)\n",
    "        #save_path = os.path.join(PERSIST_DIRECTORY, dir)\n",
    "        # incllude embedding model id in save_path\n",
    "        save_path = os.path.join(PERSIST_DIRECTORY, dir, os.path.basename(EMBEDDING_MODEL_NAME))\n",
    "        ingest.main(device_type=device_type, embedding_model=embeddings, chunk_size=chunk_size, chunk_overlap=chunk_overlap,\\\n",
    "                    source_directory=source_directory, save_path=save_path)\n",
    "\n",
    "        persist_directory = os.path.join(save_path, f'cs_{chunk_size}_co_{chunk_overlap}')\n",
    "        \n",
    "        # Getting the query from queries/feature.txt\n",
    "        with open(f\"queries/{feature}.txt\", \"r\") as f:\n",
    "            query = f.read()\n",
    "\n",
    "        print(\"Running localGPT...\")\n",
    "        answer, docs = run_localGPT.main(device_type, llm, k, persist_directory, query, use_history, verbose=False, show_sources=False)\n",
    "\n",
    "        # Saving the answer in answers/dex_name/feature/model_id/k_cs_co.txt\n",
    "        os.makedirs(f\"answers/{dex_name}/{feature}/{os.path.basename(MODEL_ID)}\", exist_ok=True)\n",
    "        with open(f\"answers/{dex_name}/{feature}/{os.path.basename(MODEL_ID)}/k_{k}_cs_{cs}_co_{co}.txt\", \"w\") as f:\n",
    "            f.write(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
