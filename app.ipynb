{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingest\n",
    "import run_localGPT\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "import os\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your GitHub personal access token\n",
    "TOKEN = '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'\n",
    "\n",
    "# Define the folder for storing database\n",
    "SOURCE_DIRECTORY = f\"tmp_data\"\n",
    "# Define the folder for storing the embeddings\n",
    "PERSIST_DIRECTORY = f\"tmp_persist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_url_as_html(url, save_path):\n",
    "    try:\n",
    "        # Send a GET request to the URL to fetch the content\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Save the content as an HTML file\n",
    "            with open(save_path, 'w', encoding='utf-8') as html_file:\n",
    "                html_file.write(response.text)\n",
    "            print(f\"HTML content saved as {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch the URL. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def get_documents(dex_name, save_path=SOURCE_DIRECTORY):\n",
    "    # Extract links for liquidity model using googlesearch (only html files)\n",
    "    query = f'{dex_name} liquidity model'\n",
    "    search_results = search(query, num_results=1)\n",
    "    liquidity_model_link = list(search_results)\n",
    "\n",
    "    # create a folder for the dex liquidity model\n",
    "    os.makedirs(f'{save_path}/{dex_name}/liquidity_model', exist_ok=True)\n",
    "\n",
    "    # save liquidity model pages as html\n",
    "    for i, link in enumerate(liquidity_model_link):\n",
    "        try:\n",
    "            save_url_as_html(link, f'{save_path}/{dex_name}/liquidity_model/{hashlib.md5(link.encode()).hexdigest()}_{i+1}.html')\n",
    "        except:\n",
    "            print(\"Could not save the page.\")\n",
    "\n",
    "    # create a folder for the dex if it doesn't exist\n",
    "    os.makedirs(f'{save_path}/{dex_name}/license', exist_ok=True)\n",
    "    # Flag to track if a license has been found for this DEX\n",
    "    license_found = False\n",
    "    \n",
    "    # Make a GitHub API repository search request based on the DEX name\n",
    "    search_url = f'https://api.github.com/search/repositories?q={dex_name}&per_page=10'\n",
    "    headers = {'Authorization': f'token {TOKEN}'}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()['items']\n",
    "\n",
    "        for repo in search_results:\n",
    "            # Check if a license file exists and retrieve the license text\n",
    "            license_url = f'https://api.github.com/repos/{repo[\"owner\"][\"login\"]}/{repo[\"name\"]}/license'\n",
    "            response = requests.get(license_url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                license_data = response.json()\n",
    "                if 'content' in license_data:\n",
    "                    license_text = base64.b64decode(license_data['content']).decode('utf-8')\n",
    "                    # Save license text to a file in the dex folder in the license folder \n",
    "                    with open(f'{save_path}/{dex_name}/license/{repo[\"full_name\"].replace(\"/\", \"__\")}.txt', 'w') as f:\n",
    "                        f.write(license_text)\n",
    "                    # Set the flag to True to indicate that a license has been found\n",
    "                    license_found = True\n",
    "                    break  # Stop searching for licenses in other repositories for this DEX\n",
    "            else:\n",
    "                print(f'Failed to fetch license for {repo[\"full_name\"]}: {response.status_code}')\n",
    "    \n",
    "        # If no official license is found, create a txt file with the message\n",
    "        if not license_found:\n",
    "            with open(f'{save_path}/{dex_name}/license/no_license.txt', 'w') as f:\n",
    "                f.write(\"No official license found for this DEX.\")\n",
    "    else:\n",
    "        print(f'Failed to search for repositories related to {dex_name}: {response.status_code}')\n",
    "    \n",
    "    # return path to the dex folder\n",
    "    return f'{save_path}/{dex_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_interaction(dex_name, k, co, cs, progress=gr.Progress()):\n",
    "    results = {}\n",
    "\n",
    "    # Check if all parameters are provided\n",
    "    if dex_name and k is not None and co is not None and cs is not None:\n",
    "        progress(0.2, desc=\"Scraping documents...\")\n",
    "        #time.sleep(1)\n",
    "        #dex_folder = get_documents(dex_name) # SOURCE_DIRECTORY/dex_name\n",
    "        dex_folder = \"tmp_data/Uniswap v3\"\n",
    "\n",
    "        progress(0.4, desc=\"Loading embedding model...\")\n",
    "        #time.sleep(1)\n",
    "        embedding_model = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-large\", model_kwargs={\"device\": \"cpu\"})\n",
    "\n",
    "        progress(0.6, desc=\"Loading LLM model...\")\n",
    "        #time.sleep(1)\n",
    "        llm = run_localGPT.load_model(device_type=\"cpu\", model_id=\"TheBloke/Llama-2-7b-Chat-GGUF\", model_basename=\"llama-2-7b-chat.Q4_K_M.gguf\")\n",
    "\n",
    "        # Define features to process\n",
    "        features = [\"liquidity_model\", \"license\"]\n",
    "        \n",
    "        for feature in features:\n",
    "            source_directory = f\"{dex_folder}/{feature}\"\n",
    "            #progress(0.5, desc=\"Ingesting documents...\")\n",
    "            #time.sleep(1)\n",
    "            #save_path = f\"{source_directory}/{embedding_model.model_name}\"\n",
    "            save_path = f\"{PERSIST_DIRECTORY}/{dex_name}/{feature}/{embedding_model.model_name.replace('/', '_')}\"\n",
    "            # Convert chunk_size and chunk_overlap to integers\n",
    "            cs = int(cs)\n",
    "            co = int(co)\n",
    "            ingest.main(device_type=\"cpu\", embedding_model=embedding_model, chunk_size=cs, chunk_overlap=co,\n",
    "                        source_directory=source_directory, save_path=save_path)\n",
    "            \n",
    "            persist_directory = os.path.join(save_path, f'cs_{cs}_co_{co}')\n",
    "\n",
    "            # Getting the query from queries/feature.txt\n",
    "            with open(f\"queries/{feature}.txt\", \"r\") as f:\n",
    "                query = f.read()\n",
    "                query = query.replace(\"the DEX\", dex_name)\n",
    "\n",
    "            # Convert k to an integer\n",
    "            k = int(k)\n",
    "\n",
    "            # Running localGPT\n",
    "            #progress(1, desc=\"Running localGPT...\")\n",
    "            #time.sleep(1)\n",
    "            answer, docs = run_localGPT.main(\"cpu\", llm, k, persist_directory, query, verbose=False, show_sources=False, promptTemplate_type=\"llama\")\n",
    "\n",
    "            # Store the results\n",
    "            results[feature] = {\"answer\": answer, \"sources\": [document.page_content for document in docs]}\n",
    "\n",
    "        # Unload the model and free up resources\n",
    "        #del llm\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution1 = \"C:/Users/mmahmoud/Pictures/chatwdoc (1).png\"\n",
    "solution2 = \"C:/Users/mmahmoud/Pictures/chatwdoc (2).png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Loading Model: TheBloke/Llama-2-7b-Chat-GGUF, on: cpu\n",
      "This action can take a few minutes!\n",
      "Using Llamacpp for GGUF/GGML quantized models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Uniswap v3/liquidity_model\n",
      "Loaded 5 documents from tmp_data/Uniswap v3/liquidity_model\n",
      "Split into 138 chunks of text\n",
      "Running on: cpu\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n",
      "Loading documents from tmp_data/Uniswap v3/license\n",
      "Loaded 1 documents from tmp_data/Uniswap v3/license\n",
      "Split into 13 chunks of text\n",
      "Running on: cpu\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Dex name\": [\"Uniswap v3\", \"SushiSwap\", \"PancakeSwap\"], \"Liquidity model\": [\"LM1\", \"LM2\", \"LM3\"], \"License\": [\"MIT\", \"Apache\", \"GPL\"]})\n",
    "\n",
    "\n",
    "with gr.Blocks(gr.themes.Default()) as demo:\n",
    "    # title\n",
    "    gr.Markdown(\"<h1 style='color: #4285F4; font-size: 36px;'>🌟 DEX Navigator</h1>\")\n",
    "    gr.Markdown(\"<p style='font-size: 20px;'>An app that helps you find answers to questions about a DEX.</p>\")\n",
    "    with gr.Tab(\"Table\"):\n",
    "        table = gr.DataFrame(df)\n",
    "    with gr.Tab(\"Interact with the app\"):\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    dex = gr.Textbox(label=\"DEX Name\", value=\"Uniswap v3\", placeholder=\"Enter the name of the DEX\")\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Temperature\", info=\"Choose between 0 and 1\")\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Top P\", info=\"Choose between 0 and 1\")\n",
    "                with gr.Column(\"Ingesting Parameters\"):\n",
    "                    cs = gr.Number(label=\"Chunk Size\", value=500) \n",
    "                    co = gr.Number(label=\"Chunk Overlap\", value=100)\n",
    "                    k = gr.Number(label=\"Number of Chunks\", minimum=1, maximum=5, value=3, info=\"Choose between 1 and 5\")              \n",
    "            with gr.Column():\n",
    "                results = gr.JSON(label=\"Results\")\n",
    "        extract_button = gr.Button(\"Extract\")\n",
    "    extract_button.click(user_interaction, inputs=[dex, k, co, cs], outputs=[results])\n",
    "\n",
    "    with gr.Tab(\"How it works ?\"):\n",
    "        gr.Gallery(label=\"Solution\", value=[solution1, solution2], columns=2, rows=1, object_fit=\"scale-down\")\n",
    "\n",
    "demo.queue(concurrency_count=20).launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
