{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('/path/to/project')\n",
    "sys.path.append(\"C:/Users/mmahmoud/localGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ingest\n",
    "import run_localGPT\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "import requests\n",
    "from googlesearch import search\n",
    "import hashlib\n",
    "import os\n",
    "import base64\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoinMarketCap_DEX_page_URL = \"https://coinmarketcap.com/rankings/exchanges/dex/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_coinmarketcap_dex_page():\n",
    "    \"\"\"\n",
    "    Scrapes the CoinMarketCap DEX page for the DEX table and writes the data to a xlsx file.\n",
    "    the table contains the name of the DEX and the link to the DEX page on CoinMarketCap.\n",
    "    url: URL of the CoinMarketCap DEX page\n",
    "    save_path: path to save the xlsx file\n",
    "    \"\"\"\n",
    "\n",
    "    # Parse the HTML content\n",
    "    html_content = requests.get(CoinMarketCap_DEX_page_URL).content\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find the table and table rows containing the DEX information\n",
    "    table = soup.find(\"table\", {\"class\": \"sc-66133f36-3 dOrjhR cmc-table\"})\n",
    "    # table's body\n",
    "    table_body = table.find(\"tbody\")\n",
    "    table_rows = table_body.find_all(\"tr\")\n",
    "\n",
    "    # Prepare the data for the table\n",
    "    table_data = []\n",
    "    for row in table_rows:\n",
    "        columns = row.find_all(\"td\")\n",
    "        if len(columns) >= 2:\n",
    "            dex_name_elem = columns[1]\n",
    "            dex_website_elem = columns[1].find(\"a\", {\"class\": \"cmc-link\"})\n",
    "            if dex_name_elem and dex_website_elem:\n",
    "                dex_name = dex_name_elem.text\n",
    "                table_data.append(dex_name)\n",
    "\n",
    "    # Write the data to a xlsx file\n",
    "    df = pd.DataFrame(table_data, columns=[\"Dex Name\"])\n",
    "\n",
    "    # Preporcessing dex names : \n",
    "    # - Uniswap v3 (Ethereum)2\t-> Uniswap v3\n",
    "    # - if the dex id from the top 10 remove the last char if it is a digit (classement)\n",
    "\n",
    "    df[\"Dex Name\"][:10] = df[\"Dex Name\"][:10].apply(lambda x: x[:-1] if x[-1].isdigit() else x)\n",
    "    df[\"Dex Name\"] = df[\"Dex Name\"].apply(lambda x: x.split(\"(\")[0].strip())\n",
    "\n",
    "    # delete duplicates based on name and show the percentage of duplicates\n",
    "    #print(\"Percentage of duplicates : \", 100 - len(df.drop_duplicates(subset=['Dex Name'], keep='first'))/len(df)*100, \"%\")\n",
    "    df = df.drop_duplicates(subset=['Dex Name'], keep='first')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dex_list = scrape_coinmarketcap_dex_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_dex_list.to_excel(\"dataframes/dex_list.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your GitHub personal access token\n",
    "TOKEN = '*********************************'\n",
    "\n",
    "# Define the folder for storing database\n",
    "SOURCE_DIRECTORY = f\"tmp_data\"\n",
    "# Define the folder for storing the embeddings\n",
    "PERSIST_DIRECTORY = f\"tmp_persist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_url_as_html(url, save_path):\n",
    "    try:\n",
    "        # Send a GET request to the URL to fetch the content\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Save the content as an HTML file\n",
    "            with open(save_path, 'w', encoding='utf-8') as html_file:\n",
    "                html_file.write(response.text)\n",
    "            print(f\"HTML content saved as {save_path}\")\n",
    "        else:\n",
    "            print(f\"Failed to fetch the URL. Status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def get_documents(dex_name, save_path=SOURCE_DIRECTORY):\n",
    "    # Extract links for liquidity model using googlesearch (only html files)\n",
    "    query = f'{dex_name} liquidity model'\n",
    "    search_results = search(query, num_results=1)\n",
    "    liquidity_model_link = list(search_results)\n",
    "\n",
    "    # create a folder for the dex liquidity model\n",
    "    os.makedirs(f'{save_path}/{dex_name}/liquidity_model', exist_ok=True)\n",
    "\n",
    "    # If no link is found, create a txt file with the message\n",
    "    if len(liquidity_model_link) == 0:\n",
    "        with open(f'{save_path}/{dex_name}/liquidity_model/no_liquidity_model.txt', 'w') as f:\n",
    "            f.write(\"No liquidity model found for this DEX.\")\n",
    "    \n",
    "    else:\n",
    "        # save liquidity model pages as html\n",
    "        for i, link in enumerate(liquidity_model_link):\n",
    "            try:\n",
    "                save_url_as_html(link, f'{save_path}/{dex_name}/liquidity_model/{hashlib.md5(link.encode()).hexdigest()}_{i+1}.html')\n",
    "            except:\n",
    "                print(\"Could not save the page.\")\n",
    "\n",
    "    # create a folder for the dex if it doesn't exist\n",
    "    os.makedirs(f'{save_path}/{dex_name}/license', exist_ok=True)\n",
    "    # Flag to track if a license has been found for this DEX\n",
    "    license_found = False\n",
    "    \n",
    "    # Make a GitHub API repository search request based on the DEX name\n",
    "    search_url = f'https://api.github.com/search/repositories?q={dex_name}&per_page=10'\n",
    "    headers = {'Authorization': f'token {TOKEN}'}\n",
    "    response = requests.get(search_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        search_results = response.json()['items']\n",
    "\n",
    "        for repo in search_results:\n",
    "            # Check if a license file exists and retrieve the license text\n",
    "            license_url = f'https://api.github.com/repos/{repo[\"owner\"][\"login\"]}/{repo[\"name\"]}/license'\n",
    "            response = requests.get(license_url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                license_data = response.json()\n",
    "                if 'content' in license_data:\n",
    "                    license_text = base64.b64decode(license_data['content']).decode('utf-8')\n",
    "                    # Save license text to a file in the dex folder in the license folder \n",
    "                    with open(f'{save_path}/{dex_name}/license/{repo[\"full_name\"].replace(\"/\", \"__\")}.txt', 'w') as f:\n",
    "                        f.write(license_text)\n",
    "                    # Set the flag to True to indicate that a license has been found\n",
    "                    license_found = True\n",
    "                    break  # Stop searching for licenses in other repositories for this DEX\n",
    "            else:\n",
    "                print(f'Failed to fetch license for {repo[\"full_name\"]}: {response.status_code}')\n",
    "    \n",
    "        # If no official license is found, create a txt file with the message\n",
    "        if not license_found:\n",
    "            with open(f'{save_path}/{dex_name}/license/no_license.txt', 'w') as f:\n",
    "                f.write(\"No official license found for this DEX.\")\n",
    "    else:\n",
    "        print(f'Failed to search for repositories related to {dex_name}: {response.status_code}')\n",
    "    \n",
    "    # return path to the dex folder\n",
    "    return f'{save_path}/{dex_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID=\"TheBloke/Mistral-7B-v0.1-GGUF\"\n",
    "MODEL_BASENAME=\"mistral-7b-v0.1.Q8_0.gguf\"\n",
    "# If llama in model_id, use promptTemplate_type=\"llama\"\n",
    "promptTemplate_type=None\n",
    "if \"llama\" in MODEL_ID.lower():\n",
    "    promptTemplate_type=\"llama\"\n",
    "elif \"mistral\" in MODEL_ID.lower():\n",
    "    promptTemplate_type=\"mistral\"\n",
    "\n",
    "EMBEDDING_NAME = \"hkunlp/instructor-large\"\n",
    "\n",
    "OPENAI_API_KEY = \"*********************************\"\n",
    "OPENAI_ORGANIZATION = \"*********************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_interaction(dex_name, k, co, cs, progress=gr.Progress()):\n",
    "    results = {}\n",
    "    # Define features to process\n",
    "    features = [\"liquidity_model\", \"license\"]\n",
    "\n",
    "    # Check if all parameters are provided\n",
    "    if dex_name and k is not None and co is not None and cs is not None:\n",
    "        progress(0.0, desc=\"Scraping documents...\")\n",
    "        # time.sleep(1)\n",
    "        # if the dex folder doesn't exist, use get_documents to scrape the documents\n",
    "        dex_folder = f\"{SOURCE_DIRECTORY}/{dex_name}\"\n",
    "        if not os.path.exists(dex_folder):\n",
    "            dex_folder = get_documents(dex_name)\n",
    "        # dex_folder = get_documents(dex_name) # SOURCE_DIRECTORY/dex_name\n",
    "        \n",
    "\n",
    "        progress(0.1, desc=\"Loading embedding model...\")\n",
    "        # time.sleep(1)\n",
    "        # embedding_model = HuggingFaceInstructEmbeddings(model_name=EMBEDDING_NAME, model_kwargs={\"device\": \"cpu\"})\n",
    "        embedding_model = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, openai_organization=OPENAI_ORGANIZATION)\n",
    "\n",
    "        progress(0.2, desc=\"Loading LLM model...\")\n",
    "        #time.sleep(1)\n",
    "        # llm = run_localGPT.load_model(device_type=\"cpu\", model_id=MODEL_ID, model_basename=MODEL_BASENAME)\n",
    "        llm = OpenAI(openai_api_key=OPENAI_API_KEY, openai_organization=OPENAI_ORGANIZATION, temperature=0.0)\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            source_directory = f\"{dex_folder}/{feature}\"\n",
    "            progress(0.4 + 0.6*(i)/len(features), desc=f\"Processing {feature}..\")\n",
    "\n",
    "            #save_path = f\"{source_directory}/{embedding_model.model_name}\"\n",
    "            save_path = f\"{source_directory}/openaiembeddings\"\n",
    "            #save_path = f\"{PERSIST_DIRECTORY}/{dex_name}/{feature}/{embedding_model.model_name.replace('/', '_')}\"\n",
    "            save_path = f\"{PERSIST_DIRECTORY}/{dex_name}/{feature}/openaiembeddings\"\n",
    "            # Convert chunk_size and chunk_overlap to integers\n",
    "            cs = int(cs)\n",
    "            co = int(co)\n",
    "            ingest.main(embedding_model=embedding_model, chunk_size=cs, chunk_overlap=co,\n",
    "                        source_directory=source_directory, save_path=save_path)\n",
    "            \n",
    "            persist_directory = os.path.join(save_path, f'cs_{cs}_co_{co}')\n",
    "\n",
    "            # Getting the query from queries/feature.txt\n",
    "            with open(f\"../queries/{feature}.txt\", \"r\") as f:\n",
    "                query = f.read()\n",
    "                query = query.replace(\"the DEX\", dex_name)\n",
    "\n",
    "            # Convert k to an integer\n",
    "            k = int(k)\n",
    "\n",
    "            # Running localGPT\n",
    "            answer, docs = run_localGPT.main(llm, embedding_model, k, persist_directory, query, verbose=False, show_sources=False, promptTemplate_type=None)\n",
    "\n",
    "            # Store the results\n",
    "            results[feature] = {\"answer\": answer, \"sources\": [document.page_content for document in docs]}\n",
    "\n",
    "            # save the results to a json file\n",
    "            results_path = f\"json/{dex_name}.json\"\n",
    "            os.makedirs(\"json\", exist_ok=True)\n",
    "            with open(results_path, \"w\") as f:\n",
    "                json.dump(results, f)\n",
    "\n",
    "        # After obtaining the results, update the DataFrame\n",
    "        \n",
    "        # Assuming you have obtained features from the results\n",
    "        features = list(results.keys())\n",
    "        new_row = {\"Dex name\": dex_name}\n",
    "\n",
    "        for feature in features:\n",
    "            format_answer = results[feature]['answer'].replace('\\n', '<br>')\n",
    "            # enumerate the sources\n",
    "            sources = [source.replace('\\n', ' ') for source in results[feature]['sources']]\n",
    "            format_sources = \"<br>\".join([f\"<span style='color: Red;'><strong>{i+1}.</strong></span> {source}\" for i, source in enumerate(sources)])\n",
    "            new_row[feature] = f\"<span style='color: green;'><strong>Answer:</strong></span> {format_answer}\\\n",
    "                <br> <span style='color: Red;'><strong>Sources:</strong></span><br> {format_sources}\"\n",
    "\n",
    "\n",
    "        # Append the new row to the DataFrame\n",
    "        df = pd.read_excel(\"dataframes/table.xlsx\")\n",
    "        # Concatenate the new row with the DataFrame if the row doesn't already exist\n",
    "        # Otherwise, update the row\n",
    "        if dex_name in df[\"Dex name\"].values:\n",
    "            for feature in features:\n",
    "                df.loc[df[\"Dex name\"] == dex_name, feature] = new_row[feature]\n",
    "        else:\n",
    "            df = pd.concat([df, pd.DataFrame(new_row, index=[0])])\n",
    "        df.to_excel(\"dataframes/table.xlsx\", index=False)\n",
    "\n",
    "        table = gr.Dataframe(\n",
    "            headers=[\"Dex name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value=pd.read_excel(\"dataframes/table.xlsx\"), \n",
    "            wrap=True\n",
    "            )\n",
    "        # Unload the model and free up resources\n",
    "        #del llm\n",
    "        dex_to_search_from_table = gr.Dropdown(\n",
    "                                    label=\"DEX Name\",\n",
    "                                    choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist(),\n",
    "                                    info=\"Choose the DEX to search from the table.\"\n",
    "                                    )\n",
    "    return table, results, dex_to_search_from_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution1 = \"images/chatwdoc (1).png\"\n",
    "solution2 = \"images/chatwdoc (2).png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table.xlsx if it doesn't exist\n",
    "if not os.path.exists(\"dataframes/table.xlsx\"):\n",
    "    df = pd.DataFrame({\"Dex name\": [], \"liquidity_model\": [], \"license\": []})\n",
    "    df.to_excel(\"dataframes/table.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def refresh_table():\n",
    "    table = gr.Dataframe(\n",
    "            headers=[\"Dex name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value=pd.read_excel(\"dataframes/table.xlsx\"), \n",
    "            wrap=True\n",
    "            )\n",
    "    return table\"\"\"\n",
    "\n",
    "def refresh_dex_list():\n",
    "    df_dex_list = pd.read_excel(\"dataframes/dex_list.xlsx\")\n",
    "    df = scrape_coinmarketcap_dex_page()\n",
    "    # add def to df_dex_list if not already present\n",
    "    #df_dex_list = df_dex_list.append(df[~df[\"Dex Name\"].isin(df_dex_list[\"Dex Name\"])])\n",
    "    # Assuming df and df_dex_list are your DataFrames\n",
    "    df_dex_list = pd.concat([df_dex_list, df[~df[\"Dex Name\"].isin(df_dex_list[\"Dex Name\"])]])\n",
    "    # Reset the index of the resulting DataFrame\n",
    "    df_dex_list.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_dex_list.to_excel(\"dataframes/dex_list.xlsx\", index=False)\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                value=\"Uniswap v3\",\n",
    "                allow_custom_value=True,\n",
    "                filterable=True)\n",
    "    return dropdown\n",
    "\n",
    "\n",
    "def delete_table(confirm_checkbox):\n",
    "    if confirm_checkbox:         \n",
    "        df = pd.DataFrame({\"Dex name\": [], \"liquidity_model\": [], \"license\": []})\n",
    "        df.to_excel(\"dataframes/table.xlsx\", index=False)\n",
    "    else:\n",
    "        gr.Info(\"Please confirm that you want to delete the table.\")\n",
    "    table = gr.Dataframe(\n",
    "            headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value=pd.read_excel(\"dataframes/table.xlsx\"), \n",
    "            wrap=True\n",
    "            )\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist())\n",
    "    dex_to_search_from_table = gr.Dropdown(\n",
    "                                    label=\"DEX Name\",\n",
    "                                    choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist(),\n",
    "                                    info=\"Choose the DEX to search from the table.\"\n",
    "                                    )\n",
    "    return dex_to_search_from_table, dropdown, table\n",
    "\n",
    "def delete_dex_from_table(dex_to_delete_from_table, confirm_delete_dex_from_table):\n",
    "    if not dex_to_delete_from_table:\n",
    "        gr.Info(\"Please select a DEX to delete from the table.\")\n",
    "    else:\n",
    "        if not confirm_delete_dex_from_table:\n",
    "            gr.Info(\"Please confirm that you want to delete the DEX from the table.\")\n",
    "        else:\n",
    "            df = pd.read_excel(\"dataframes/table.xlsx\")\n",
    "            df = df[df[\"Dex name\"] != dex_to_delete_from_table]\n",
    "            df.to_excel(\"dataframes/table.xlsx\", index=False)\n",
    "    table = gr.Dataframe(\n",
    "            headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value = pd.read_excel(\"dataframes/table.xlsx\"), \n",
    "            wrap=True,\n",
    "            height=1000\n",
    "        )\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist())\n",
    "    \n",
    "    dex_to_search_from_table = gr.Dropdown(\n",
    "                                    label=\"DEX Name\",\n",
    "                                    choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist(),\n",
    "                                    info=\"Choose the DEX to search from the table.\"\n",
    "                                    )\n",
    "    return dex_to_search_from_table, dropdown, table\n",
    "\n",
    "def delete_dex_from_dropdown(dex_to_delete_from_dropdown, confirm_delete_dex_from_dropdown):\n",
    "    if not dex_to_delete_from_dropdown:\n",
    "        gr.Info(\"Please select a DEX to delete from the dropdown.\")\n",
    "    else:\n",
    "        if not confirm_delete_dex_from_dropdown:\n",
    "            gr.Info(\"Please confirm that you want to delete the DEX from the dropdown.\")\n",
    "        else:\n",
    "            df = pd.read_excel(\"dataframes/dex_list.xlsx\")\n",
    "            df = df[df[\"Dex Name\"] != dex_to_delete_from_dropdown]\n",
    "            df.to_excel(\"dataframes/dex_list.xlsx\", index=False)\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                value=\"Uniswap v3\",\n",
    "                allow_custom_value=True,\n",
    "                filterable=True)\n",
    "    dropdown1 = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist()\n",
    "                )\n",
    "    return dropdown, dropdown1\n",
    "\n",
    "def delete_dropdown_list(confirm_delete_dropdown_list):\n",
    "    if not confirm_delete_dropdown_list:\n",
    "        gr.Info(\"Please confirm that you want to delete the dropdown list.\")\n",
    "    else:\n",
    "        df = pd.DataFrame({\"Dex Name\": []})\n",
    "        df.to_excel(\"dataframes/dex_list.xlsx\", index=False)\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                value=\"Uniswap v3\",\n",
    "                allow_custom_value=True,\n",
    "                filterable=True)\n",
    "    dropdown1 = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist()\n",
    "                )\n",
    "    return dropdown, dropdown1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_and_extract_all(k, co, cs, progress=gr.Progress()):\n",
    "    results = gr.JSON(label=\"Results\")\n",
    "    old_list=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist()\n",
    "    #dropdown = refresh_dex_list()\n",
    "    #updated_list=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist()\n",
    "    #new_list = list(set(updated_list) - set(old_list))\n",
    "    #if len(new_list) == 0:\n",
    "        #gr.Info(\"No new DEXs added to the dropdown.\")\n",
    "    # Show the new DEXs in the dropdown\n",
    "    #else :\n",
    "        #gr.Info(f\"New DEXs added to the dropdown: {', '.join(new_list)}\")\n",
    "    for dex_name in progress.tqdm(old_list):\n",
    "        _, results, _ = user_interaction(dex_name, k, co, cs)\n",
    "    dropdown = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                value=\"Uniswap v3\",\n",
    "                allow_custom_value=True,\n",
    "                filterable=True)\n",
    "    dropdown1 = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist()\n",
    "                )\n",
    "    dropdown2 = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist())\n",
    "    dropdown3 = gr.Dropdown(\n",
    "                label=\"DEX Name\",\n",
    "                choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist())\n",
    "    \n",
    "    table = gr.Dataframe(\n",
    "            headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value = pd.read_excel(\"dataframes/table.xlsx\"),\n",
    "            wrap=True,\n",
    "            height=1000\n",
    "        )\n",
    "    return dropdown, dropdown1, dropdown2, dropdown3, results, table\n",
    "\n",
    "def search_fn(dex_name):\n",
    "    dex_row = pd.DataFrame({\"Dex name\": [], \"liquidity_model\": [], \"license\": []})\n",
    "    visible = False\n",
    "    if not dex_name:\n",
    "        gr.Info(\"Please select a DEX to search for.\")\n",
    "    else:\n",
    "        visible = True\n",
    "        dex_row = pd.read_excel(\"dataframes/table.xlsx\")[pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"] == dex_name]\n",
    "    return gr.Column(visible=visible), gr.DataFrame(\n",
    "                    headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "                    datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "                    value=dex_row,\n",
    "                    wrap=True,\n",
    "                    height=1000\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete dex names into the dropdown list that are in the table\n",
    "#df_dex_list = pd.read_excel(\"dataframes/dex_list.xlsx\")\n",
    "#df_table = pd.read_excel(\"dataframes/table.xlsx\")\n",
    "#df_dex_list = df_dex_list[~df_dex_list[\"Dex Name\"].isin(df_table[\"Dex name\"])]\n",
    "#df_dex_list.to_excel(\"dataframes/dex_list.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Noah/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Noah/liquidity_model\n",
      "Split into 157 chunks of text\n",
      "Loading documents from tmp_data/Noah/license\n",
      "Loaded 1 documents from tmp_data/Noah/license\n",
      "Split into 35 chunks of text\n",
      "Loading documents from tmp_data/Sharkyswap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Sharkyswap/liquidity_model\n",
      "Split into 41 chunks of text\n",
      "Loading documents from tmp_data/Sharkyswap/license\n",
      "Loaded 1 documents from tmp_data/Sharkyswap/license\n",
      "Split into 103 chunks of text\n",
      "Loading documents from tmp_data/Symmetric/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Symmetric/liquidity_model\n",
      "Split into 185 chunks of text\n",
      "Loading documents from tmp_data/Symmetric/license\n",
      "Loaded 1 documents from tmp_data/Symmetric/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Kwikswap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Kwikswap/liquidity_model\n",
      "Split into 123 chunks of text\n",
      "Loading documents from tmp_data/Kwikswap/license\n",
      "Loaded 1 documents from tmp_data/Kwikswap/license\n",
      "Split into 103 chunks of text\n",
      "Loading documents from tmp_data/DefySwap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/DefySwap/liquidity_model\n",
      "Split into 31 chunks of text\n",
      "Loading documents from tmp_data/DefySwap/license\n",
      "Loaded 1 documents from tmp_data/DefySwap/license\n",
      "Split into 5 chunks of text\n",
      "Loading documents from tmp_data/Greenhouse/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Greenhouse/liquidity_model\n",
      "Split into 27 chunks of text\n",
      "Loading documents from tmp_data/Greenhouse/license\n",
      "Loaded 1 documents from tmp_data/Greenhouse/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Diffusion Finance/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Diffusion Finance/liquidity_model\n",
      "Split into 567 chunks of text\n",
      "Loading documents from tmp_data/Diffusion Finance/license\n",
      "Loaded 1 documents from tmp_data/Diffusion Finance/license\n",
      "Split into 76 chunks of text\n",
      "Loading documents from tmp_data/NeutroSwap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/NeutroSwap/liquidity_model\n",
      "Split into 179 chunks of text\n",
      "Loading documents from tmp_data/NeutroSwap/license\n",
      "Loaded 1 documents from tmp_data/NeutroSwap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/CantoSwap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/CantoSwap/liquidity_model\n",
      "Split into 104 chunks of text\n",
      "Loading documents from tmp_data/CantoSwap/license\n",
      "Loaded 1 documents from tmp_data/CantoSwap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Sterling Finance/liquidity_model\n",
      "Loaded 1 documents from tmp_data/Sterling Finance/liquidity_model\n",
      "Split into 77 chunks of text\n",
      "Loading documents from tmp_data/Sterling Finance/license\n",
      "Loaded 1 documents from tmp_data/Sterling Finance/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/KAIDEX/liquidity_model\n",
      "Loaded 2 documents from tmp_data/KAIDEX/liquidity_model\n",
      "Split into 22 chunks of text\n",
      "Loading documents from tmp_data/KAIDEX/license\n",
      "Loaded 1 documents from tmp_data/KAIDEX/license\n",
      "Split into 3 chunks of text\n",
      "HTML content saved as tmp_data/Woof Finance/liquidity_model/5f596c4763e9e532dde40087d3ca090d_1.html\n",
      "Failed to fetch the URL. Status code: 403\n",
      "Failed to fetch license for WooFinance/WooFinance.github.io: 404\n",
      "Failed to fetch license for samtechelectronics/chowchowfinanceFrontEnd: 404\n",
      "Loading documents from tmp_data/Woof Finance/liquidity_model\n",
      "Loaded 1 documents from tmp_data/Woof Finance/liquidity_model\n",
      "Split into 31 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a0048c7244115888b0b80e7d1488e20 in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a0048c7244115888b0b80e7d1488e20 in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 2a0048c7244115888b0b80e7d1488e20 in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 25 Oct 2023 12:19:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-eevdrq3snz5fhxyha8nnwr6l', 'openai-processing-ms': '30033', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '997555', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '146ms', 'x-request-id': '2a0048c7244115888b0b80e7d1488e20', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '81ba679cbab3f120-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Woof Finance/license\n",
      "Loaded 1 documents from tmp_data/Woof Finance/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/EvmoSwap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/EvmoSwap/liquidity_model\n",
      "Split into 116 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d187e07ea5f2240de1b959722e828aaa in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d187e07ea5f2240de1b959722e828aaa in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID d187e07ea5f2240de1b959722e828aaa in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 25 Oct 2023 12:20:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-eevdrq3snz5fhxyha8nnwr6l', 'openai-processing-ms': '30009', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '999818', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '10ms', 'x-request-id': 'd187e07ea5f2240de1b959722e828aaa', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '81ba691329faf120-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/EvmoSwap/license\n",
      "Loaded 1 documents from tmp_data/EvmoSwap/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Daylight Protocol/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Daylight Protocol/liquidity_model\n",
      "Split into 977 chunks of text\n",
      "Loading documents from tmp_data/Daylight Protocol/license\n",
      "Loaded 1 documents from tmp_data/Daylight Protocol/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Archly Finance/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Archly Finance/liquidity_model\n",
      "Split into 12 chunks of text\n",
      "Loading documents from tmp_data/Archly Finance/license\n",
      "Loaded 1 documents from tmp_data/Archly Finance/license\n",
      "Split into 5 chunks of text\n",
      "Loading documents from tmp_data/IslandSwap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/IslandSwap/liquidity_model\n",
      "Split into 57 chunks of text\n",
      "Loading documents from tmp_data/IslandSwap/license\n",
      "Loaded 1 documents from tmp_data/IslandSwap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Decaswap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Decaswap/liquidity_model\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Decaswap/license\n",
      "Loaded 1 documents from tmp_data/Decaswap/license\n",
      "Split into 103 chunks of text\n",
      "Loading documents from tmp_data/FlameSwap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/FlameSwap/liquidity_model\n",
      "Split into 23 chunks of text\n",
      "Loading documents from tmp_data/FlameSwap/license\n",
      "Loaded 1 documents from tmp_data/FlameSwap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Cronus Finance/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Cronus Finance/liquidity_model\n",
      "Split into 44 chunks of text\n",
      "Loading documents from tmp_data/Cronus Finance/license\n",
      "Loaded 1 documents from tmp_data/Cronus Finance/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML content saved as tmp_data/LeetSwap/liquidity_model/83f619e8bd1a763c4421ead632280bcb_1.html\n",
      "Failed to fetch the URL. Status code: 410\n",
      "HTML content saved as tmp_data/LeetSwap/liquidity_model/74a21b5137b6911f61158fbfeb2257d0_3.html\n",
      "Loading documents from tmp_data/LeetSwap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/LeetSwap/liquidity_model\n",
      "Split into 33 chunks of text\n",
      "Loading documents from tmp_data/LeetSwap/license\n",
      "Loaded 1 documents from tmp_data/LeetSwap/license\n",
      "Split into 103 chunks of text\n",
      "Loading documents from tmp_data/Kwenta/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Kwenta/liquidity_model\n",
      "Split into 32 chunks of text\n",
      "Loading documents from tmp_data/Kwenta/license\n",
      "Loaded 1 documents from tmp_data/Kwenta/license\n",
      "Split into 103 chunks of text\n",
      "Loading documents from tmp_data/CoinSwap Space/liquidity_model\n",
      "Loaded 3 documents from tmp_data/CoinSwap Space/liquidity_model\n",
      "Split into 22 chunks of text\n",
      "Loading documents from tmp_data/CoinSwap Space/license\n",
      "Loaded 1 documents from tmp_data/CoinSwap Space/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/ZilSwap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/ZilSwap/liquidity_model\n",
      "Split into 18 chunks of text\n",
      "Loading documents from tmp_data/ZilSwap/license\n",
      "Loaded 1 documents from tmp_data/ZilSwap/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Hebeswap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Hebeswap/liquidity_model\n",
      "Split into 18 chunks of text\n",
      "Loading documents from tmp_data/Hebeswap/license\n",
      "Loaded 1 documents from tmp_data/Hebeswap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/NSKSwap/liquidity_model\n",
      "Loaded 2 documents from tmp_data/NSKSwap/liquidity_model\n",
      "Split into 22 chunks of text\n",
      "Loading documents from tmp_data/NSKSwap/license\n",
      "Loaded 1 documents from tmp_data/NSKSwap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/onAVAX/liquidity_model\n",
      "Loaded 3 documents from tmp_data/onAVAX/liquidity_model\n",
      "Split into 75 chunks of text\n",
      "Loading documents from tmp_data/onAVAX/license\n",
      "Loaded 1 documents from tmp_data/onAVAX/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Apeswap/liquidity_model\n",
      "Loaded 4 documents from tmp_data/Apeswap/liquidity_model\n",
      "Split into 230 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.embeddings.openai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised APIError: The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4633134af78d0c22218fa0b01a07f79d in your message.) {\n",
      "  \"error\": {\n",
      "    \"message\": \"The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4633134af78d0c22218fa0b01a07f79d in your message.)\",\n",
      "    \"type\": \"server_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": null\n",
      "  }\n",
      "}\n",
      " 500 {'error': {'message': 'The server had an error while processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4633134af78d0c22218fa0b01a07f79d in your message.)', 'type': 'server_error', 'param': None, 'code': None}} {'Date': 'Wed, 25 Oct 2023 12:25:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '366', 'Connection': 'keep-alive', 'access-control-allow-origin': '*', 'openai-organization': 'user-eevdrq3snz5fhxyha8nnwr6l', 'openai-processing-ms': '30222', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15724800; includeSubDomains', 'x-ratelimit-limit-requests': '3000', 'x-ratelimit-limit-tokens': '1000000', 'x-ratelimit-remaining-requests': '2999', 'x-ratelimit-remaining-tokens': '975555', 'x-ratelimit-reset-requests': '20ms', 'x-ratelimit-reset-tokens': '1.466s', 'x-request-id': '4633134af78d0c22218fa0b01a07f79d', 'CF-Cache-Status': 'DYNAMIC', 'Server': 'cloudflare', 'CF-RAY': '81ba71051b8a999e-CDG', 'alt-svc': 'h3=\":443\"; ma=86400'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/Apeswap/license\n",
      "Loaded 2 documents from tmp_data/Apeswap/license\n",
      "Split into 104 chunks of text\n",
      "Loading documents from tmp_data/ALEX/liquidity_model\n",
      "Loaded 3 documents from tmp_data/ALEX/liquidity_model\n",
      "Split into 151 chunks of text\n",
      "Loading documents from tmp_data/ALEX/license\n",
      "Loaded 1 documents from tmp_data/ALEX/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Oasis Swap/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Oasis Swap/liquidity_model\n",
      "Split into 61 chunks of text\n",
      "Loading documents from tmp_data/Oasis Swap/license\n",
      "Loaded 1 documents from tmp_data/Oasis Swap/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/NESTFi/liquidity_model\n",
      "Loaded 3 documents from tmp_data/NESTFi/liquidity_model\n",
      "Split into 33 chunks of text\n",
      "Loading documents from tmp_data/NESTFi/license\n",
      "Loaded 1 documents from tmp_data/NESTFi/license\n",
      "Split into 3 chunks of text\n",
      "Loading documents from tmp_data/Merlin DEX/liquidity_model\n",
      "Loaded 2 documents from tmp_data/Merlin DEX/liquidity_model\n",
      "Split into 53 chunks of text\n",
      "Loading documents from tmp_data/Merlin DEX/license\n",
      "Loaded 1 documents from tmp_data/Merlin DEX/license\n",
      "Split into 1 chunks of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from tmp_data/ko.one/liquidity_model\n",
      "Loaded 3 documents from tmp_data/ko.one/liquidity_model\n",
      "Split into 41 chunks of text\n",
      "Loading documents from tmp_data/ko.one/license\n",
      "Loaded 1 documents from tmp_data/ko.one/license\n",
      "Split into 31 chunks of text\n",
      "Loading documents from tmp_data/Purple Bridge/liquidity_model\n",
      "Loaded 3 documents from tmp_data/Purple Bridge/liquidity_model\n",
      "Split into 153 chunks of text\n",
      "Loading documents from tmp_data/Purple Bridge/license\n",
      "Loaded 1 documents from tmp_data/Purple Bridge/license\n",
      "Split into 103 chunks of text\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks(theme=gr.themes.Default(primary_hue='indigo', secondary_hue='orange')) as demo:\n",
    "    # title\n",
    "    gr.Markdown(\"<h1 style='color: #6C63FF; font-size: 40px; text-align: center;'>DEX explorer</h1>\")\n",
    "    gr.Markdown(\"<p style='font-size: 20px; text-align: center; margin-top: 10px;'>An app that helps you find answers to questions about a DEX.</p>\")\n",
    "\n",
    "    with gr.Tab(\"Table\"):\n",
    "        table = gr.Dataframe(\n",
    "            headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "            datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            value = pd.read_excel(\"dataframes/table.xlsx\"), \n",
    "            wrap=True,\n",
    "            height=1000\n",
    "        )\n",
    "        #refresh_button = gr.Button(\"Refresh Table\")\n",
    "        #refresh_button.click(refresh_table, outputs=[table], show_progress=True)\n",
    "    with gr.Tab(\"Search\"):\n",
    "        with gr.Row():\n",
    "            dex_to_search_from_table = gr.Dropdown(\n",
    "                                    label=\"DEX Name\",\n",
    "                                    choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist(),\n",
    "                                    info=\"Choose the DEX to search from the table.\"\n",
    "                                    )\n",
    "            search_button = gr.Button(\"Search\", variant='primary')\n",
    "        with gr.Column(visible=False) as search_column:\n",
    "            search_result = gr.DataFrame(\n",
    "                headers=[\"DEX name\", \"liquidity_model\", \"license\"],\n",
    "                datatype=[\"str\", \"markdown\", \"markdown\"],\n",
    "            )\n",
    "        search_button.click(search_fn, inputs=[dex_to_search_from_table], outputs=[search_column, search_result], show_progress=True)\n",
    "\n",
    "    with gr.Tab(\"Interact with the app\"):\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        dex = gr.Dropdown(\n",
    "                            label=\"DEX Name\", \n",
    "                            choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                            value=\"Uniswap v3\",\n",
    "                            allow_custom_value=True,\n",
    "                            filterable=True\n",
    "                            )\n",
    "                        update_dex_list_button = gr.Button(\"Update DEX List\", variant='primary')\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Temperature\", info=\"Choose between 0 and 1\")\n",
    "                    gr.Slider(minimum=0, maximum=1, value=0, label=\"Top P\", info=\"Choose between 0 and 1\")\n",
    "                with gr.Column(\"Ingesting Parameters\"):\n",
    "                    cs = gr.Number(label=\"Chunk Size\", value=500) \n",
    "                    co = gr.Number(label=\"Chunk Overlap\", value=100)\n",
    "                    k = gr.Number(label=\"Number of Chunks\", minimum=1, maximum=5, value=3, info=\"Choose between 1 and 5\")            \n",
    "            with gr.Column():\n",
    "                results = gr.JSON(label=\"Results\")\n",
    "        with gr.Row():\n",
    "            extract_button = gr.Button(\"Extract\", variant='primary')\n",
    "            update_and_extract_all_button = gr.Button(\"Update and Extract All\", variant='primary')\n",
    "\n",
    "        extract_button.click(user_interaction, inputs=[dex, k, co, cs], outputs=[table, results, dex_to_search_from_table])\n",
    "        update_dex_list_button.click(refresh_dex_list, outputs=[dex], show_progress=True)\n",
    "\n",
    "    # Add a new advanced features tab\n",
    "    # Theses features are : deleting a dex name from the dropdown list, deleting a dex_name from the table\n",
    "    # deleting the entire table, deleting the entire dropdown list\n",
    "    with gr.Tab(\"Delete\"):\n",
    "        with gr.Row():\n",
    "            dex_to_delete_from_dropdown = gr.Dropdown(\n",
    "                                        label=\"DEX Name\",\n",
    "                                        choices=pd.read_excel(\"dataframes/dex_list.xlsx\")[\"Dex Name\"].tolist(),\n",
    "                                        info=\"Choose the DEX to delete from the dropdown list.\"\n",
    "                                        )\n",
    "            with gr.Column():\n",
    "                confirm_delete_dex_from_dropdown = gr.Checkbox(\n",
    "                                                    label=\"Confirm\",\n",
    "                                                    info=\"This will delete the DEX from the dropdown \\\n",
    "                                                        list and cannot be undone. By checking this box, \\\n",
    "                                                        you confirm that you want to delete the DEX from the dropdown list.\"\n",
    "                                                    )\n",
    "                delete_dex_from_dropdown_button = gr.Button(\"Delete DEX from dropdown list\", variant='primary')\n",
    "        with gr.Row():\n",
    "            dex_to_delete_from_table = gr.Dropdown(\n",
    "                                    label=\"DEX Name\",\n",
    "                                    choices=pd.read_excel(\"dataframes/table.xlsx\")[\"Dex name\"].tolist(),\n",
    "                                    info=\"Choose the DEX to delete from the table.\"\n",
    "                                    )\n",
    "            with gr.Column():\n",
    "                confirm_delete_dex_from_table = gr.Checkbox(\n",
    "                                                label=\"Confirm\",\n",
    "                                                info=\"This will delete the DEX from the table and \\\n",
    "                                                    cannot be undone. By checking this box, you \\\n",
    "                                                    confirm that you want to delete the DEX from the table.\"\n",
    "                                                    )\n",
    "                delete_dex_from_table_button = gr.Button(\"Delete DEX from table\", variant='primary')\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                confirm_delete_table = gr.Checkbox(label=\"Confirm\", info=\"This will delete the table and cannot be undone. By checking this box, you confirm that you want to delete the table.\")\n",
    "                delete_table_button = gr.Button(\"Delete Table\", variant='primary')\n",
    "            with gr.Column():\n",
    "                confirm_delete_dropdown_list = gr.Checkbox(label=\"Confirm\", info=\"This will delete the dropdown list and cannot be undone. By checking this box, you confirm that you want to delete the dropdown list.\")\n",
    "                delete_dropdown_list_button = gr.Button(\"Delete Dropdown List\", variant='primary')\n",
    "        \n",
    "        delete_dex_from_table_button.click(delete_dex_from_table, inputs=[dex_to_delete_from_table, confirm_delete_dex_from_table], outputs=[dex_to_search_from_table, dex_to_delete_from_table, table], show_progress=True)\n",
    "        delete_dex_from_dropdown_button.click(delete_dex_from_dropdown, inputs=[dex_to_delete_from_dropdown, confirm_delete_dex_from_dropdown], outputs=[dex, dex_to_delete_from_dropdown], show_progress=True)\n",
    "        delete_dropdown_list_button.click(delete_dropdown_list, inputs=[confirm_delete_dropdown_list], outputs=[dex, dex_to_delete_from_dropdown], show_progress=True)\n",
    "        delete_table_button.click(delete_table, inputs=[confirm_delete_table], outputs=[dex_to_search_from_table, dex_to_delete_from_table, table], show_progress=True)\n",
    "\n",
    "    update_and_extract_all_button.click(\n",
    "            update_and_extract_all,\n",
    "            inputs=[k, co, cs],\n",
    "            outputs=[dex, dex_to_delete_from_dropdown, dex_to_delete_from_table, dex_to_search_from_table, results, table],\n",
    "            show_progress=True\n",
    "            )\n",
    "\n",
    "    with gr.Tab(\"How does it work?\"):\n",
    "        gr.Gallery(label=\"Solution\", value=[solution1, solution2], columns=2, rows=1, object_fit=\"scale-down\")\n",
    "\n",
    "demo.queue(concurrency_count=20).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
